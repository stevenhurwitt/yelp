{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark read s3 json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported modules.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window, Row, SparkSession\n",
    "\n",
    "import pprint\n",
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent = 3)\n",
    "print('imported modules.')\n",
    "\n",
    "# Set Java home environment variable\n",
    "# os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/temurin-8.jdk/Contents/Home'  # Update this path to match your Java installation\n",
    "\n",
    "# read creds.json\n",
    "with open(\"creds.json\", \"r\") as f:\n",
    "    creds = json.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop any existing Spark session\n",
    "if 'spark' in locals():\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/11 17:55:03 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/11 17:56:09 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:600)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/11 17:56:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:600)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "25/03/11 17:56:10 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:600)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/11 17:56:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:600)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "25/03/11 17:56:20 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:600)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/11 17:56:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:600)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "25/03/11 17:56:30 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:600)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/11 17:56:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:600)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "25/03/11 17:56:40 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:600)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/11 17:56:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:600)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "25/03/11 17:56:50 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:600)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/11 17:56:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:600)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "25/03/11 17:57:00 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:600)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/11 17:57:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:600)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Create Spark session with required configurations\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"YelpAnalysis\") \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", creds[\"aws_client\"]) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", creds[\"aws_secret\"]) \\\n",
    "        .config(\"spark.jars.packages\", \n",
    "                \"org.apache.hadoop:hadoop-aws:3.3.4,\" + \n",
    "                \"org.apache.hadoop:hadoop-common:3.3.4,\" +\n",
    "                \"org.apache.logging.log4j:log4j-slf4j-impl:2.17.2,\" +\n",
    "                \"org.apache.logging.log4j:log4j-api:2.17.2,\" +\n",
    "                \"org.apache.logging.log4j:log4j-core:2.17.2,\" + \n",
    "                \"org.apache.hadoop:hadoop-client:3.3.4,\" + \n",
    "                \"io.delta:delta-core_2.12:2.4.0,\" + \n",
    "                \"org.postgresql:postgresql:9.4.1212\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "        # .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-common:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-client:3.3.4,io.delta:delta-core_2.12:2.4.0,org.postgresql:postgresql:9.4.1212\") \\\n",
    "        \n",
    "    \n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list s3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files in s3 bucket:\n",
      "\n",
      "yelp_academic_dataset_business.json\n",
      "yelp_academic_dataset_checkin.json\n",
      "yelp_academic_dataset_review.json\n",
      "yelp_academic_dataset_tip.json\n",
      "yelp_academic_dataset_user.json\n"
     ]
    }
   ],
   "source": [
    "client = boto3.client('s3',\n",
    "                    aws_access_key_id=creds[\"aws_client\"],\n",
    "                    aws_secret_access_key= creds[\"aws_secret\"])\n",
    "bucket = \"yelp-stevenhurwitt-2\"\n",
    "file = \"yelp_academic_dataset_business.json\"\n",
    "\n",
    "bucket_meta = client.list_objects(Bucket = 'yelp-stevenhurwitt-2')\n",
    "print('files in s3 bucket:')\n",
    "print('')\n",
    "for c in bucket_meta['Contents']:\n",
    "    print(c['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark read json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schemas for each dataset\n",
    "checkin_schema = StructType([\n",
    "    StructField(\"business_id\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True)\n",
    "])\n",
    "\n",
    "review_schema = StructType([\n",
    "    StructField(\"review_id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"business_id\", StringType(), True),\n",
    "    StructField(\"stars\", FloatType(), True),\n",
    "    StructField(\"useful\", IntegerType(), True),\n",
    "    StructField(\"funny\", IntegerType(), True),\n",
    "    StructField(\"cool\", IntegerType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "tip_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"business_id\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"date\", TimestampType(), True),\n",
    "    StructField(\"compliment_count\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "user_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"review_count\", IntegerType(), True),\n",
    "    StructField(\"yelping_since\", TimestampType(), True),\n",
    "    StructField(\"friends\", StringType(), True),\n",
    "    StructField(\"useful\", IntegerType(), True),\n",
    "    StructField(\"funny\", IntegerType(), True),\n",
    "    StructField(\"cool\", IntegerType(), True),\n",
    "    StructField(\"fans\", IntegerType(), True),\n",
    "    StructField(\"elite\", StringType(), True),\n",
    "    StructField(\"average_stars\", FloatType(), True),\n",
    "    StructField(\"compliment_hot\", IntegerType(), True),\n",
    "    StructField(\"compliment_more\", IntegerType(), True),\n",
    "    StructField(\"compliment_profile\", IntegerType(), True),\n",
    "    StructField(\"compliment_cute\", IntegerType(), True),\n",
    "    StructField(\"compliment_list\", IntegerType(), True),\n",
    "    StructField(\"compliment_note\", IntegerType(), True),\n",
    "    StructField(\"compliment_plain\", IntegerType(), True),\n",
    "    StructField(\"compliment_cool\", IntegerType(), True),\n",
    "    StructField(\"compliment_funny\", IntegerType(), True),\n",
    "    StructField(\"compliment_writer\", IntegerType(), True),\n",
    "    StructField(\"compliment_photos\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "business_schema = StructType([\n",
    "    StructField(\"business_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"postal_code\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"stars\", DoubleType(), True),\n",
    "    StructField(\"review_count\", IntegerType(), True),\n",
    "    StructField(\"is_open\", IntegerType(), True),\n",
    "    StructField(\"attributes\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"categories\", StringType(), True),\n",
    "    StructField(\"hours\", MapType(StringType(), StringType()), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load yelp dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataframes\n",
    "def load_yelp_dataframes(base_path):\n",
    "    try:\n",
    "        # Load each dataset\n",
    "        df_business = spark.read.schema(business_schema)\\\n",
    "            .json(f\"{base_path}/yelp_academic_dataset_business.json\")\n",
    "        \n",
    "        df_checkin = spark.read.schema(checkin_schema)\\\n",
    "            .json(f\"{base_path}/yelp_academic_dataset_checkin.json\")\n",
    "        \n",
    "        df_review = spark.read.schema(review_schema)\\\n",
    "            .json(f\"{base_path}/yelp_academic_dataset_review.json\")\n",
    "        \n",
    "        df_tip = spark.read.schema(tip_schema)\\\n",
    "            .json(f\"{base_path}/yelp_academic_dataset_tip.json\")\n",
    "        \n",
    "        df_user = spark.read.schema(user_schema)\\\n",
    "            .json(f\"{base_path}/yelp_academic_dataset_user.json\")\n",
    "        \n",
    "        # Print basic info about loaded dataframes\n",
    "        print(f\"Businesses: {df_business.count()} rows\")\n",
    "        print(f\"Checkins: {df_checkin.count()} rows\")\n",
    "        print(f\"Reviews: {df_review.count()} rows\")\n",
    "        print(f\"Tips: {df_tip.count()} rows\")\n",
    "        print(f\"Users: {df_user.count()} rows\")\n",
    "        \n",
    "        return df_business, df_checkin, df_review, df_tip, df_user\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataframes: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data\n",
    "\n",
    "http://localhost:4040/jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Businesses: 150346 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkins: 131930 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews: 6990280 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: 908915 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:====================================>                   (17 + 4) / 26]\r"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "base_path = \"s3a://yelp-stevenhurwitt-2/\"  # Update this path\n",
    "df_business, df_checkin, df_review, df_tip, df_user = load_yelp_dataframes(base_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verify schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify schemas\n",
    "print(\"\\nCheckin Schema:\")\n",
    "df_checkin.printSchema()\n",
    "print(\"\\nReview Schema:\")\n",
    "df_review.printSchema()\n",
    "print(\"\\nTip Schema:\")\n",
    "df_tip.printSchema()\n",
    "print(\"\\nUser Schema:\")\n",
    "df_user.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write to delta on s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_s3_delta(df, bucket, folder, partition_cols=None):\n",
    "    \"\"\"\n",
    "    Write DataFrame to S3 in Delta format\n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "        bucket: S3 bucket name\n",
    "        folder: Folder name for the dataset\n",
    "        partition_cols: List of columns to partition by (optional)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        writer = df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "        \n",
    "        if partition_cols:\n",
    "            writer = writer.partitionBy(partition_cols)\n",
    "            \n",
    "        path = f\"s3a://{bucket}/{folder}/\"\n",
    "        writer.save(path)\n",
    "        print(f\"Successfully wrote {folder} data to Delta table at {path}\")\n",
    "        \n",
    "        # Create Delta table if it doesn't exist\n",
    "        spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS delta.`{path}`\n",
    "        USING DELTA\n",
    "        \"\"\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing {folder} to Delta: {str(e)}\")\n",
    "\n",
    "# Usage example:\n",
    "bucket = \"yelp-stevenhurwitt-2\"\n",
    "\n",
    "try:\n",
    "    # Reviews - partition by year and month\n",
    "    df_review = df_review.withColumn(\n",
    "        \"year\", year(\"date\")\n",
    "    ).withColumn(\n",
    "        \"month\", month(\"date\")\n",
    "    )\n",
    "    write_to_s3_delta(df_review, bucket, \"reviews\", [\"year\", \"month\"])\n",
    "\n",
    "    # Users - no partitioning\n",
    "    write_to_s3_delta(df_user, bucket, \"users\")\n",
    "\n",
    "    # Tips - partition by year\n",
    "    df_tip = df_tip.withColumn(\"year\", year(\"date\"))\n",
    "    write_to_s3_delta(df_tip, bucket, \"tips\", [\"year\"])\n",
    "\n",
    "    # Checkins - no partitioning\n",
    "    write_to_s3_delta(df_checkin, bucket, \"checkins\")\n",
    "\n",
    "    print(\"All datasets written to Delta format successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in write operations: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Delta table\n",
    "df = spark.read.format(\"delta\").load(\"s3a://yelp-stevenhurwitt-2/reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write to postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Business table\n",
    "# CREATE TABLE IF NOT EXISTS business (\n",
    "#     business_id STRING,\n",
    "#     name STRING,\n",
    "#     address STRING,\n",
    "#     city STRING,\n",
    "#     state STRING,\n",
    "#     postal_code STRING,\n",
    "#     latitude DOUBLE,\n",
    "#     longitude DOUBLE,\n",
    "#     stars FLOAT,\n",
    "#     review_count INT,\n",
    "#     is_open INT,\n",
    "#     attributes MAP<STRING, STRING>,\n",
    "#     categories STRING,\n",
    "#     hours MAP<STRING, STRING>\n",
    "# ) USING DELTA\n",
    "# LOCATION 's3a://yelp-stevenhurwitt-2/business';\n",
    "\n",
    "# -- Review table\n",
    "# CREATE TABLE IF NOT EXISTS review (\n",
    "#     review_id STRING,\n",
    "#     user_id STRING,\n",
    "#     business_id STRING,\n",
    "#     stars FLOAT,\n",
    "#     useful INT,\n",
    "#     funny INT,\n",
    "#     cool INT,\n",
    "#     text STRING,\n",
    "#     date TIMESTAMP,\n",
    "#     year INT,\n",
    "#     month INT\n",
    "# ) USING DELTA\n",
    "# LOCATION 's3a://yelp-stevenhurwitt-2/reviews';\n",
    "\n",
    "# -- User table\n",
    "# CREATE TABLE IF NOT EXISTS user (\n",
    "#     user_id STRING,\n",
    "#     name STRING,\n",
    "#     review_count INT,\n",
    "#     yelping_since TIMESTAMP,\n",
    "#     friends STRING,\n",
    "#     useful INT,\n",
    "#     funny INT,\n",
    "#     cool INT,\n",
    "#     fans INT,\n",
    "#     elite STRING,\n",
    "#     average_stars FLOAT,\n",
    "#     compliment_hot INT,\n",
    "#     compliment_more INT,\n",
    "#     compliment_profile INT,\n",
    "#     compliment_cute INT,\n",
    "#     compliment_list INT,\n",
    "#     compliment_note INT,\n",
    "#     compliment_plain INT,\n",
    "#     compliment_cool INT,\n",
    "#     compliment_funny INT,\n",
    "#     compliment_writer INT,\n",
    "#     compliment_photos INT\n",
    "# ) USING DELTA\n",
    "# LOCATION 's3a://yelp-stevenhurwitt-2/users';\n",
    "\n",
    "# -- Tip table\n",
    "# CREATE TABLE IF NOT EXISTS tip (\n",
    "#     user_id STRING,\n",
    "#     business_id STRING,\n",
    "#     text STRING,\n",
    "#     date TIMESTAMP,\n",
    "#     compliment_count INT,\n",
    "#     year INT\n",
    "# ) USING DELTA\n",
    "# LOCATION 's3a://yelp-stevenhurwitt-2/tips';\n",
    "\n",
    "# -- Checkin table\n",
    "# CREATE TABLE IF NOT EXISTS checkin (\n",
    "#     business_id STRING,\n",
    "#     date STRING\n",
    "# ) USING DELTA\n",
    "# LOCATION 's3a://yelp-stevenhurwitt-2/checkins';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write to postgres from spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_postgres(df, table_name):\n",
    "    \"\"\"\n",
    "    Write Spark DataFrame to PostgreSQL\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "        table_name: Name of the target PostgreSQL table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # PostgreSQL connection properties\n",
    "        postgres_props = {\n",
    "            \"url\": f\"jdbc:postgresql://{creds['postgres_host']}:5433/{creds['postgres_db']}\",\n",
    "            \"driver\": \"org.postgresql.Driver\",\n",
    "            \"user\": creds[\"postgres_user\"],\n",
    "            \"password\": creds[\"postgres_password\"]\n",
    "        }\n",
    "        \n",
    "        # Write DataFrame to PostgreSQL\n",
    "        df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", postgres_props[\"url\"]) \\\n",
    "            .option(\"driver\", postgres_props[\"driver\"]) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"user\", postgres_props[\"user\"]) \\\n",
    "            .option(\"password\", postgres_props[\"password\"]) \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "            \n",
    "        print(f\"Successfully wrote {table_name} to PostgreSQL\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing {table_name} to PostgreSQL: {str(e)}\")\n",
    "\n",
    "# Write all datasets to PostgreSQL\n",
    "try:\n",
    "    # Add PostgreSQL JDBC driver to Spark\n",
    "    spark.sparkContext.addJar(\"postgresql-42.2.23.jar\")\n",
    "    \n",
    "    # Write each dataset\n",
    "    write_to_postgres(df_business, \"business\")\n",
    "    write_to_postgres(df_review, \"review\")\n",
    "    write_to_postgres(df_user, \"user_profile\")  # using user_profile since user is reserved\n",
    "    write_to_postgres(df_tip, \"tip\")\n",
    "    write_to_postgres(df_checkin, \"checkin\")\n",
    "    \n",
    "    print(\"All datasets written to PostgreSQL successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in PostgreSQL write operations: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    \"\"\"\n",
    "    reads a yelp .json file from s3 bucket.\n",
    "\n",
    "    keyword arguments:\n",
    "    filename - name of file (str)\n",
    "\n",
    "    returns: json_file (json)\n",
    "    \"\"\"\n",
    "\n",
    "    bucket = \"yelp-dataset-stevenhurwitt\"\n",
    "    file = \"raw/yelp_academic_dataset_business.json\"\n",
    "    response = client.get_object(Bucket = bucket, Key = file)\n",
    "    \n",
    "    file_content = response['Body'].read().decode('utf-8')\n",
    "    json_file = json.loads(\"[\" + file_content.replace(\"}\\n{\", \"},\\n{\") + \"]\")\n",
    "    return(json_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read json files from s3.\n"
     ]
    }
   ],
   "source": [
    "business_file = read_json(\"raw/yelp_academic_dataset_business.json\")\n",
    "checkin_file = read_json(\"raw/yelp_academic_dataset_checkin.json\")\n",
    "review_file = read_json(\"raw/yelp_academic_dataset_review.json\")\n",
    "tip_file = read_json(\"raw/yelp_academic_dataset_tip.json\")\n",
    "user_file = read_json(\"raw/yelp_academic_dataset_user.json\")\n",
    "\n",
    "print(\"read json files from s3.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  'address': '1616 Chapala St, Ste 2',\n",
      "   'attributes': {'ByAppointmentOnly': 'True'},\n",
      "   'business_id': 'Pns2l4eNsfO8kk83dixA6A',\n",
      "   'categories': 'Doctors, Traditional Chinese Medicine, '\n",
      "                 'Naturopathic/Holistic, Acupuncture, Health & Medical, '\n",
      "                 'Nutritionists',\n",
      "   'city': 'Santa Barbara',\n",
      "   'hours': None,\n",
      "   'is_open': 0,\n",
      "   'latitude': 34.4266787,\n",
      "   'longitude': -119.7111968,\n",
      "   'name': 'Abby Rappoport, LAC, CMQ',\n",
      "   'postal_code': '93101',\n",
      "   'review_count': 7,\n",
      "   'stars': 5.0,\n",
      "   'state': 'CA'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(review_file[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  'address': '1616 Chapala St, Ste 2',\n",
      "   'attributes': {'ByAppointmentOnly': 'True'},\n",
      "   'business_id': 'Pns2l4eNsfO8kk83dixA6A',\n",
      "   'categories': 'Doctors, Traditional Chinese Medicine, '\n",
      "                 'Naturopathic/Holistic, Acupuncture, Health & Medical, '\n",
      "                 'Nutritionists',\n",
      "   'city': 'Santa Barbara',\n",
      "   'hours': None,\n",
      "   'is_open': 0,\n",
      "   'latitude': 34.4266787,\n",
      "   'longitude': -119.7111968,\n",
      "   'name': 'Abby Rappoport, LAC, CMQ',\n",
      "   'postal_code': '93101',\n",
      "   'review_count': 7,\n",
      "   'stars': 5.0,\n",
      "   'state': 'CA'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(tip_file[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  'address': '1616 Chapala St, Ste 2',\n",
      "   'attributes': {'ByAppointmentOnly': 'True'},\n",
      "   'business_id': 'Pns2l4eNsfO8kk83dixA6A',\n",
      "   'categories': 'Doctors, Traditional Chinese Medicine, '\n",
      "                 'Naturopathic/Holistic, Acupuncture, Health & Medical, '\n",
      "                 'Nutritionists',\n",
      "   'city': 'Santa Barbara',\n",
      "   'hours': None,\n",
      "   'is_open': 0,\n",
      "   'latitude': 34.4266787,\n",
      "   'longitude': -119.7111968,\n",
      "   'name': 'Abby Rappoport, LAC, CMQ',\n",
      "   'postal_code': '93101',\n",
      "   'review_count': 7,\n",
      "   'stars': 5.0,\n",
      "   'state': 'CA'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(user_file[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checkin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  'address': '1616 Chapala St, Ste 2',\n",
      "   'attributes': {'ByAppointmentOnly': 'True'},\n",
      "   'business_id': 'Pns2l4eNsfO8kk83dixA6A',\n",
      "   'categories': 'Doctors, Traditional Chinese Medicine, '\n",
      "                 'Naturopathic/Holistic, Acupuncture, Health & Medical, '\n",
      "                 'Nutritionists',\n",
      "   'city': 'Santa Barbara',\n",
      "   'hours': None,\n",
      "   'is_open': 0,\n",
      "   'latitude': 34.4266787,\n",
      "   'longitude': -119.7111968,\n",
      "   'name': 'Abby Rappoport, LAC, CMQ',\n",
      "   'postal_code': '93101',\n",
      "   'review_count': 7,\n",
      "   'stars': 5.0,\n",
      "   'state': 'CA'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(checkin_file[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  'address': '1616 Chapala St, Ste 2',\n",
      "   'attributes': {'ByAppointmentOnly': 'True'},\n",
      "   'business_id': 'Pns2l4eNsfO8kk83dixA6A',\n",
      "   'categories': 'Doctors, Traditional Chinese Medicine, '\n",
      "                 'Naturopathic/Holistic, Acupuncture, Health & Medical, '\n",
      "                 'Nutritionists',\n",
      "   'city': 'Santa Barbara',\n",
      "   'hours': None,\n",
      "   'is_open': 0,\n",
      "   'latitude': 34.4266787,\n",
      "   'longitude': -119.7111968,\n",
      "   'name': 'Abby Rappoport, LAC, CMQ',\n",
      "   'postal_code': '93101',\n",
      "   'review_count': 7,\n",
      "   'stars': 5.0,\n",
      "   'state': 'CA'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(business_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dynamo resource.\n"
     ]
    }
   ],
   "source": [
    "dynamodb = boto3.resource('dynamodb', endpoint_url=\"https://us-east-2.console.aws.amazon.com?arn=arn:aws:dynamodb:us-east-2134132211607:8000\")\n",
    "print('created dynamo resource.')\n",
    "\n",
    "# try:\n",
    "#     yelp_business = dynamodb.create_table(\n",
    "#             TableName='yelp.business',\n",
    "#             KeySchema=[\n",
    "#                 {\n",
    "#                     'AttributeName': 'business_id',\n",
    "#                     'KeyType': 'HASH'  # Partition key\n",
    "#                 }\n",
    "#             ],\n",
    "#             AttributeDefinitions=[\n",
    "#                 {\n",
    "#                     'AttributeName': 'name',\n",
    "#                     'AttributeType': 'S'\n",
    "#                 }\n",
    "#             ],\n",
    "#             ProvisionedThroughput={\n",
    "#                 'ReadCapacityUnits': 25,\n",
    "#                 'WriteCapacityUnits': 20\n",
    "#             }\n",
    "#         )\n",
    "#     print('created dynamo table.')\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"exception: {e}\")\n",
    "#     print(\"failed to create dynamo table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp-dataset-stevenhurwitt\n"
     ]
    }
   ],
   "source": [
    "print(bucket) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw/yelp_academic_dataset_business.json\n"
     ]
    }
   ],
   "source": [
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150346\n",
      "150346\n"
     ]
    }
   ],
   "source": [
    "print(len(business_file))\n",
    "print(len(checkin_file))\n",
    "# review_file = read_json(\"raw/yelp_academic_dataset_review.json\")\n",
    "# tip_file = read_json(\"raw/yelp_academic_dataset_tip.json\")\n",
    "# user_file = read_json(\"raw/yelp_academic_dataset_user.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business json file has 150346 records with size of 1.28316 mb.\n",
      "tip json file has 150346 records with size of 1.28316 mb.\n",
      "user json file has 150346 records with size of 1.28316 mb.\n",
      "review json file has 150346 records with size of 1.28316 mb.\n",
      "checkin json file has 150346 records with size of 1.28316 mb.\n"
     ]
    }
   ],
   "source": [
    "print('business json file has {} records with size of {} mb.'.format(len(business_file), sys.getsizeof(business_file)/1000000))\n",
    "print('tip json file has {} records with size of {} mb.'.format(len(tip_file), sys.getsizeof(tip_file)/1000000))\n",
    "print('user json file has {} records with size of {} mb.'.format(len(user_file), sys.getsizeof(user_file)/1000000))\n",
    "print('review json file has {} records with size of {} mb.'.format(len(review_file), sys.getsizeof(review_file)/1000000))\n",
    "print('checkin json file has {} records with size of {} mb.'.format(len(checkin_file), sys.getsizeof(checkin_file)/1000000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas = business.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_df = df_pandas.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(html_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gcloud auth login --no-launch-browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
