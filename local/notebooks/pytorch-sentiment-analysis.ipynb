{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554ab5ac",
   "metadata": {},
   "source": [
    "# PyTorch Neural Network for Yelp Review Star Rating Prediction\n",
    "\n",
    "This notebook demonstrates how to train a neural network using PyTorch to predict star ratings (1-5) based on review text from the Yelp dataset.\n",
    "\n",
    "## Overview\n",
    "We'll build a text classification model that:\n",
    "1. Takes review text as input\n",
    "2. Processes the text using NLP techniques\n",
    "3. Predicts the star rating (1-5 stars)\n",
    "\n",
    "## Steps\n",
    "1. **Data Loading & Preprocessing**\n",
    "2. **Text Tokenization & Vectorization**\n",
    "3. **Neural Network Architecture**\n",
    "4. **Training Loop**\n",
    "5. **Model Evaluation**\n",
    "6. **Inference & Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bc4a6f",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb9cb293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Libraries imported successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Spark for data loading\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download NLTK data (run once)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97930362",
   "metadata": {},
   "source": [
    "## 2. Load Data from Spark/Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34d54469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load credentials\n",
    "with open(\"creds.json\", \"r\") as f:\n",
    "    creds = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "print(\"Credentials loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9a2173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.apache.hadoop#hadoop-common added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.logging.log4j#log4j-slf4j-impl added as a dependency\n",
      "org.apache.logging.log4j#log4j-api added as a dependency\n",
      "org.apache.logging.log4j#log4j-core added as a dependency\n",
      "org.apache.hadoop#hadoop-client added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ed032f87-8e64-4840-a70f-8c2121be79d2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.hadoop#hadoop-common;3.3.4 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.4 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound com.google.guava#guava;27.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound commons-io#commons-io;2.8.0 in central\n",
      "\tfound commons-net#commons-net;3.6 in central\n",
      "\tfound commons-collections#commons-collections;3.2.2 in central\n",
      "\tfound javax.servlet#javax.servlet-api;3.1.0 in central\n",
      "\tfound org.eclipse.jetty#jetty-server;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-http;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-io;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-servlet;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-security;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-webapp;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-xml;9.4.43.v20210629 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.19 in central\n",
      "\tfound javax.ws.rs#jsr311-api;1.1.1 in central\n",
      "\tfound com.sun.jersey#jersey-servlet;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.19 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound ch.qos.reload4j#reload4j;1.2.22 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.4 in central\n",
      "\tfound org.apache.commons#commons-configuration2;2.1.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.12.0 in central\n",
      "\tfound org.apache.commons#commons-text;1.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound org.slf4j#slf4j-reload4j;1.7.36 in central\n",
      "\tfound org.apache.avro#avro;1.7.7 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.apache.commons#commons-compress;1.21 in central\n",
      "\tfound com.google.re2j#re2j;1.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;3.3.4 in central\n",
      "\tfound com.nimbusds#nimbus-jose-jwt;9.8.1 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound net.minidev#json-smart;2.4.7 in central\n",
      "\tfound net.minidev#accessors-smart;2.4.7 in central\n",
      "\tfound org.ow2.asm#asm;5.0.4 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.5.6 in central\n",
      "\tfound org.apache.zookeeper#zookeeper-jute;3.5.6 in central\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in central\n",
      "\tfound org.apache.curator#curator-framework;4.2.0 in central\n",
      "\tfound org.apache.curator#curator-client;4.2.0 in central\n",
      "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-client;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-config;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-core;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-pkix;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-asn1;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-common;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-crypto;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#token-provider;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-admin;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-server;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-identity;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-xdr;1.0.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.55 in central\n",
      "\tfound org.apache.curator#curator-recipes;4.2.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.7 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.7 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.7 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;5.3.0 in central\n",
      "\tfound dnsjava#dnsjava;2.1.7 in central\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound org.apache.logging.log4j#log4j-slf4j-impl;2.17.2 in central\n",
      "\tfound org.apache.logging.log4j#log4j-core;2.17.2 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api;2.17.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-client;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-hdfs-client;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-api;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-client;3.3.4 in central\n",
      "\tfound org.eclipse.jetty.websocket#websocket-client;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-client;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty.websocket#websocket-common;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty.websocket#websocket-api;9.4.43.v20210629 in central\n",
      "\tfound org.jline#jline;3.9.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-core;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-common;3.3.4 in central\n",
      "\tfound com.sun.jersey#jersey-client;1.19 in central\n",
      "\tfound com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.12.7 in central\n",
      "\tfound jakarta.xml.bind#jakarta.xml.bind-api;2.3.2 in central\n",
      "\tfound com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.12.7 in central\n",
      "\tfound com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.12.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-jobclient;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-common;3.3.4 in central\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/2.5.2/checker-qual-2.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.checkerframework#checker-qual;2.5.2!checker-qual.jar (419ms)\n",
      ":: resolution report :: resolve 1111ms :: artifacts dl 449ms\n",
      "\t:: modules in use:\n",
      "\tch.qos.reload4j#reload4j;1.2.22 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;5.3.0 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from central in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.1 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.55 from central in [default]\n",
      "\tcom.nimbusds#nimbus-jose-jwt;9.8.1 from central in [default]\n",
      "\tcom.sun.jersey#jersey-client;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-servlet;1.19 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.4 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.6 from central in [default]\n",
      "\tdnsjava#dnsjava;2.1.7 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from central in [default]\n",
      "\tjakarta.xml.bind#jakarta.xml.bind-api;2.3.2 from central in [default]\n",
      "\tjavax.servlet#javax.servlet-api;3.1.0 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.ws.rs#jsr311-api;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tnet.minidev#accessors-smart;2.4.7 from central in [default]\n",
      "\tnet.minidev#json-smart;2.4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.7 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.21 from central in [default]\n",
      "\torg.apache.commons#commons-configuration2;2.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.12.0 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.4 from central in [default]\n",
      "\torg.apache.curator#curator-client;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-framework;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;4.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-hdfs-client;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-mapreduce-client-common;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-mapreduce-client-core;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-mapreduce-client-jobclient;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-client;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-common;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.kerby#kerb-admin;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-client;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-common;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-core;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-crypto;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-identity;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-server;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-asn1;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-config;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-pkix;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-xdr;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#token-provider;1.0.1 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api;2.17.2 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-core;2.17.2 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-slf4j-impl;2.17.2 from central in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.5.6 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper-jute;3.5.6 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-client;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-http;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-io;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-security;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-server;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-servlet;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-webapp;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-xml;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty.websocket#websocket-api;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty.websocket#websocket-client;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty.websocket#websocket-common;9.4.43.v20210629 from central in [default]\n",
      "\torg.jline#jline;3.9.0 from central in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\torg.slf4j#slf4j-reload4j;1.7.36 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.261 by [com.amazonaws#aws-java-sdk-bundle;1.12.262] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.25 by [org.slf4j#slf4j-api;1.7.36] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  121  |   0   |   0   |   2   ||  119  |   1   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ed032f87-8e64-4840-a70f-8c2121be79d2\n",
      "\tconfs: [default]\n",
      "\t1 artifacts copied, 118 already retrieved (188kB/13ms)\n",
      "25/08/28 01:25:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/28 01:25:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "/usr/local/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"YelpPyTorchML\") \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.worker.memory\", \"4g\") \\\n",
    "        .config(\"spark.cores.max\", \"8\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", creds[\"aws_client\"]) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", creds[\"aws_secret\"]) \\\n",
    "        .config(\"spark.jars.packages\", \n",
    "                \"org.apache.hadoop:hadoop-aws:3.3.4,\" + \n",
    "                \"org.apache.hadoop:hadoop-common:3.3.4,\" +\n",
    "                \"com.amazonaws:aws-java-sdk-bundle:1.12.261,\" +\n",
    "                \"org.apache.logging.log4j:log4j-slf4j-impl:2.17.2,\" +\n",
    "                \"org.apache.logging.log4j:log4j-api:2.17.2,\" +\n",
    "                \"org.apache.logging.log4j:log4j-core:2.17.2,\" + \n",
    "                \"org.apache.hadoop:hadoop-client:3.3.4,\" + \n",
    "                \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"Spark session initialized successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Spark: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef38a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/28 01:26:31 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/08/28 01:26:38 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 4:===========>                                            (60 + 4) / 288]\r"
     ]
    }
   ],
   "source": [
    "# Load review data from Delta tables\n",
    "bucket = \"yelp-stevenhurwitt-2\"\n",
    "\n",
    "# Load reviews (sample for manageable training)\n",
    "reviews_df = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(f\"s3a://{bucket}/reviews\") \\\n",
    "    .select(\"text\", \"stars\") \\\n",
    "    .filter(col(\"text\").isNotNull() & (col(\"text\") != \"\")) \\\n",
    "    .filter(col(\"stars\").isNotNull()) \\\n",
    "    .sample(0.1, seed=42)  # Sample 10% for faster training\n",
    "\n",
    "print(f\"Loaded {reviews_df.count():,} reviews\")\n",
    "reviews_df.groupBy(\"stars\").count().orderBy(\"stars\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d245fdf",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b99c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas for easier text processing\n",
    "# Take a smaller sample for initial training\n",
    "data_pandas = reviews_df.sample(0.3, seed=42).toPandas()  # Further sample to 3% of total\n",
    "print(f\"Working with {len(data_pandas):,} reviews\")\n",
    "\n",
    "# Display class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(data_pandas['stars'].value_counts().sort_index())\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample reviews:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nStars: {data_pandas.iloc[i]['stars']}\")\n",
    "    print(f\"Text: {data_pandas.iloc[i]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07eed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing functions\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenize text and remove stopwords\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing text...\")\n",
    "data_pandas['clean_text'] = data_pandas['text'].apply(clean_text)\n",
    "data_pandas['tokens'] = data_pandas['clean_text'].apply(tokenize_text)\n",
    "\n",
    "# Remove empty reviews\n",
    "data_pandas = data_pandas[data_pandas['tokens'].apply(len) > 0]\n",
    "print(f\"After preprocessing: {len(data_pandas):,} reviews\")\n",
    "\n",
    "# Show sample processed data\n",
    "print(\"\\nSample processed data:\")\n",
    "sample_idx = 0\n",
    "print(f\"Original: {data_pandas.iloc[sample_idx]['text'][:100]}...\")\n",
    "print(f\"Cleaned: {data_pandas.iloc[sample_idx]['clean_text'][:100]}...\")\n",
    "print(f\"Tokens: {data_pandas.iloc[sample_idx]['tokens'][:15]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f0afee",
   "metadata": {},
   "source": [
    "## 4. Build Vocabulary and Create Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e5e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "def build_vocabulary(tokenized_texts, min_freq=2, max_vocab=10000):\n",
    "    \"\"\"Build vocabulary from tokenized texts\"\"\"\n",
    "    # Count word frequencies\n",
    "    word_counts = Counter()\n",
    "    for tokens in tokenized_texts:\n",
    "        word_counts.update(tokens)\n",
    "    \n",
    "    # Filter by minimum frequency and limit vocabulary size\n",
    "    vocab_words = [word for word, count in word_counts.most_common(max_vocab) \n",
    "                   if count >= min_freq]\n",
    "    \n",
    "    # Create word to index mapping\n",
    "    word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for i, word in enumerate(vocab_words):\n",
    "        word_to_idx[word] = i + 2\n",
    "    \n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    \n",
    "    return word_to_idx, idx_to_word, word_counts\n",
    "\n",
    "# Build vocabulary\n",
    "word_to_idx, idx_to_word, word_counts = build_vocabulary(\n",
    "    data_pandas['tokens'], \n",
    "    min_freq=5,  # Only include words that appear at least 5 times\n",
    "    max_vocab=10000  # Limit vocabulary to top 10k words\n",
    ")\n",
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "print(f\"Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"Most common words: {list(word_counts.most_common(10))}\")\n",
    "print(f\"Sample word indices: {[(word, idx) for word, idx in list(word_to_idx.items())[:10]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9380b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to sequences of indices\n",
    "def text_to_sequence(tokens, word_to_idx, max_length=200):\n",
    "    \"\"\"Convert tokens to sequence of indices\"\"\"\n",
    "    sequence = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in tokens]\n",
    "    \n",
    "    # Truncate or pad to max_length\n",
    "    if len(sequence) > max_length:\n",
    "        sequence = sequence[:max_length]\n",
    "    else:\n",
    "        sequence.extend([word_to_idx['<PAD>']] * (max_length - len(sequence)))\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "# Convert all texts to sequences\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "print(f\"Converting texts to sequences (max length: {MAX_SEQUENCE_LENGTH})...\")\n",
    "\n",
    "data_pandas['sequence'] = data_pandas['tokens'].apply(\n",
    "    lambda tokens: text_to_sequence(tokens, word_to_idx, MAX_SEQUENCE_LENGTH)\n",
    ")\n",
    "\n",
    "# Convert stars to 0-indexed classes (1-5 stars -> 0-4 classes)\n",
    "data_pandas['label'] = data_pandas['stars'] - 1\n",
    "\n",
    "print(\"Sample sequence:\")\n",
    "sample_tokens = data_pandas.iloc[0]['tokens'][:10]\n",
    "sample_sequence = data_pandas.iloc[0]['sequence'][:10]\n",
    "print(f\"Tokens: {sample_tokens}\")\n",
    "print(f\"Indices: {sample_sequence}\")\n",
    "print(f\"Label: {data_pandas.iloc[0]['label']} (original stars: {data_pandas.iloc[0]['stars']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59dc662",
   "metadata": {},
   "source": [
    "## 5. Create PyTorch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e52c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class YelpReviewDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sequence': torch.tensor(self.sequences[idx], dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X = data_pandas['sequence'].tolist()\n",
    "y = data_pandas['label'].tolist()\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train):,} samples\")\n",
    "print(f\"Validation set: {len(X_val):,} samples\")\n",
    "print(f\"Test set: {len(X_test):,} samples\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = YelpReviewDataset(X_train, y_train)\n",
    "val_dataset = YelpReviewDataset(X_val, y_val)\n",
    "test_dataset = YelpReviewDataset(X_test, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nData loaders created with batch size: {BATCH_SIZE}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c1410e",
   "metadata": {},
   "source": [
    "## 6. Define Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ed274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Model\n",
    "class YelpSentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers=2, dropout=0.3):\n",
    "        super(YelpSentimentClassifier, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            n_layers, \n",
    "            batch_first=True, \n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)  # *2 for bidirectional\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Use the last hidden state (from both directions)\n",
    "        # hidden shape: (n_layers * 2, batch_size, hidden_dim)\n",
    "        # We want the last layer's hidden states from both directions\n",
    "        final_hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)  # Concatenate forward and backward\n",
    "        \n",
    "        # Apply dropout\n",
    "        final_hidden = self.dropout(final_hidden)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        output = self.relu(self.fc1(final_hidden))\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Model parameters\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 5  # 5 classes (1-5 stars)\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# Initialize model\n",
    "model = YelpSentimentClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model created and moved to {device}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTrainable parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fb3a9d",
   "metadata": {},
   "source": [
    "## 7. Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073998c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "print(\"Loss function and optimizer initialized\")\n",
    "print(f\"Criterion: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "print(f\"Scheduler: {scheduler}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79e7d4d",
   "metadata": {},
   "source": [
    "## 8. Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15097825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Move data to device\n",
    "        sequences = batch['sequence'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print progress\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Batch {batch_idx}/{len(data_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Validation function\n",
    "def validate_epoch(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            sequences = batch['sequence'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"Training and validation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30277d6e",
   "metadata": {},
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6f5f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "NUM_EPOCHS = 10\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "best_val_accuracy = 0\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'best_yelp_model.pth')\n",
    "        print(f\"New best validation accuracy: {best_val_accuracy:.4f} - Model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e18f025",
   "metadata": {},
   "source": [
    "## 10. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "ax1.plot(epochs_range, train_losses, 'b-', label='Training Loss')\n",
    "ax1.plot(epochs_range, val_losses, 'r-', label='Validation Loss')\n",
    "ax1.set_title('Model Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(epochs_range, train_accuracies, 'b-', label='Training Accuracy')\n",
    "ax2.plot(epochs_range, val_accuracies, 'r-', label='Validation Accuracy')\n",
    "ax2.set_title('Model Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(f\"Final Training Accuracy: {train_accuracies[-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracies[-1]:.4f}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f21d428",
   "metadata": {},
   "source": [
    "## 11. Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc255b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for testing\n",
    "model.load_state_dict(torch.load('best_yelp_model.pth'))\n",
    "print(\"Best model loaded for testing\")\n",
    "\n",
    "# Test the model\n",
    "test_loss, test_accuracy = validate_epoch(model, test_loader, criterion, device)\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a30d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation with classification report and confusion matrix\n",
    "def detailed_evaluation(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            sequences = batch['sequence'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return all_predictions, all_labels\n",
    "\n",
    "# Get predictions\n",
    "test_predictions, test_labels = detailed_evaluation(model, test_loader, device)\n",
    "\n",
    "# Convert back to 1-5 star scale for interpretation\n",
    "test_predictions_stars = [pred + 1 for pred in test_predictions]\n",
    "test_labels_stars = [label + 1 for label in test_labels]\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    test_labels_stars, \n",
    "    test_predictions_stars, \n",
    "    target_names=['1 Star', '2 Stars', '3 Stars', '4 Stars', '5 Stars']\n",
    "))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_labels_stars, test_predictions_stars)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['1 Star', '2 Stars', '3 Stars', '4 Stars', '5 Stars'],\n",
    "            yticklabels=['1 Star', '2 Stars', '3 Stars', '4 Stars', '5 Stars'])\n",
    "plt.title('Confusion Matrix - Yelp Review Star Prediction')\n",
    "plt.xlabel('Predicted Stars')\n",
    "plt.ylabel('True Stars')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693b6c8e",
   "metadata": {},
   "source": [
    "## 12. Inference Function for New Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9eb00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict star rating for new review text\n",
    "def predict_stars(text, model, word_to_idx, device, max_length=200):\n",
    "    \"\"\"Predict star rating for a new review text\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess the text\n",
    "    clean_text_input = clean_text(text)\n",
    "    tokens = tokenize_text(clean_text_input)\n",
    "    \n",
    "    # Convert to sequence\n",
    "    sequence = text_to_sequence(tokens, word_to_idx, max_length)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    sequence_tensor = torch.tensor([sequence], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(sequence_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        _, predicted_class = torch.max(outputs, 1)\n",
    "        \n",
    "    # Convert back to 1-5 star scale\n",
    "    predicted_stars = predicted_class.item() + 1\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    # Get probability distribution\n",
    "    prob_dist = probabilities[0].cpu().numpy()\n",
    "    \n",
    "    return predicted_stars, confidence, prob_dist\n",
    "\n",
    "# Test with sample reviews\n",
    "test_reviews = [\n",
    "    \"This restaurant is absolutely amazing! The food was incredible and the service was perfect. I will definitely come back!\",\n",
    "    \"The food was okay, nothing special. Service was a bit slow but not terrible.\",\n",
    "    \"Worst experience ever! The food was cold, the service was rude, and the place was dirty. Never coming back!\",\n",
    "    \"Great atmosphere and delicious food. The staff was friendly and attentive. Highly recommended!\",\n",
    "    \"Average place. Food was decent but overpriced. Nothing to write home about.\"\n",
    "]\n",
    "\n",
    "print(\"Testing model predictions on sample reviews:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, review in enumerate(test_reviews, 1):\n",
    "    predicted_stars, confidence, prob_dist = predict_stars(review, model, word_to_idx, device)\n",
    "    \n",
    "    print(f\"\\nReview {i}:\")\n",
    "    print(f\"Text: {review}\")\n",
    "    print(f\"Predicted Stars: {predicted_stars}\")\n",
    "    print(f\"Confidence: {confidence:.3f}\")\n",
    "    print(f\"Probability Distribution:\")\n",
    "    for j, prob in enumerate(prob_dist):\n",
    "        print(f\"  {j+1} stars: {prob:.3f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ebe4a3",
   "metadata": {},
   "source": [
    "## 13. Model Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b900f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance by star rating\n",
    "from collections import defaultdict\n",
    "\n",
    "# Group predictions by true star rating\n",
    "performance_by_star = defaultdict(list)\n",
    "for true_label, prediction in zip(test_labels_stars, test_predictions_stars):\n",
    "    performance_by_star[true_label].append(prediction)\n",
    "\n",
    "print(\"Model Performance by Star Rating:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for star_rating in sorted(performance_by_star.keys()):\n",
    "    predictions = performance_by_star[star_rating]\n",
    "    correct = sum(1 for pred in predictions if pred == star_rating)\n",
    "    total = len(predictions)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    print(f\"{star_rating} Star Reviews:\")\n",
    "    print(f\"  Total: {total}\")\n",
    "    print(f\"  Correct: {correct}\")\n",
    "    print(f\"  Accuracy: {accuracy:.3f}\")\n",
    "    \n",
    "    # Show distribution of predictions\n",
    "    pred_dist = Counter(predictions)\n",
    "    print(f\"  Prediction distribution: {dict(sorted(pred_dist.items()))}\")\n",
    "    print()\n",
    "\n",
    "# Overall statistics\n",
    "overall_accuracy = accuracy_score(test_labels_stars, test_predictions_stars)\n",
    "print(f\"Overall Test Accuracy: {overall_accuracy:.4f}\")\n",
    "\n",
    "# Calculate accuracy within 1 star (close predictions)\n",
    "within_one_star = sum(1 for true, pred in zip(test_labels_stars, test_predictions_stars) \n",
    "                     if abs(true - pred) <= 1) / len(test_labels_stars)\n",
    "print(f\"Accuracy within 1 star: {within_one_star:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c408f05f",
   "metadata": {},
   "source": [
    "## 14. Save Model and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d104e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model, vocabulary, and metadata for future use\n",
    "import pickle\n",
    "\n",
    "# Save vocabulary\n",
    "with open('yelp_vocabulary.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'word_to_idx': word_to_idx,\n",
    "        'idx_to_word': idx_to_word,\n",
    "        'vocab_size': vocab_size,\n",
    "        'max_sequence_length': MAX_SEQUENCE_LENGTH\n",
    "    }, f)\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_params': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'hidden_dim': HIDDEN_DIM,\n",
    "        'output_dim': OUTPUT_DIM,\n",
    "        'n_layers': N_LAYERS,\n",
    "        'dropout': DROPOUT\n",
    "    },\n",
    "    'training_params': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'num_epochs': len(train_losses),\n",
    "        'learning_rate': 0.001,\n",
    "        'max_sequence_length': MAX_SEQUENCE_LENGTH\n",
    "    },\n",
    "    'performance': {\n",
    "        'best_val_accuracy': best_val_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'within_one_star_accuracy': within_one_star\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('model_metadata.json', 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(\"Model artifacts saved:\")\n",
    "print(\"- best_yelp_model.pth (model weights)\")\n",
    "print(\"- yelp_vocabulary.pkl (vocabulary)\")\n",
    "print(\"- model_metadata.json (model configuration and performance)\")\n",
    "\n",
    "print(f\"\\nFinal Model Summary:\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Accuracy within 1 star: {within_one_star:.4f}\")\n",
    "print(f\"Total trainable parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82448c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "\n",
    "1. **Load and preprocess** Yelp review data from Spark/Delta tables\n",
    "2. **Clean and tokenize** text data for neural network input\n",
    "3. **Build vocabulary** and convert text to numerical sequences\n",
    "4. **Create custom PyTorch Dataset** and DataLoader classes\n",
    "5. **Define a bidirectional LSTM** neural network architecture\n",
    "6. **Implement training and validation** loops with early stopping\n",
    "7. **Evaluate model performance** with detailed metrics\n",
    "8. **Create inference functions** for new text predictions\n",
    "9. **Save the trained model** and vocabulary for future use\n",
    "\n",
    "### Key Techniques Used:\n",
    "- **Bidirectional LSTM** for capturing context in both directions\n",
    "- **Word embeddings** to represent text as dense vectors\n",
    "- **Dropout and gradient clipping** for regularization\n",
    "- **Early stopping** to prevent overfitting\n",
    "- **Learning rate scheduling** for better convergence\n",
    "\n",
    "### Potential Improvements:\n",
    "- Use pre-trained word embeddings (Word2Vec, GloVe)\n",
    "- Implement attention mechanisms\n",
    "- Try transformer-based models (BERT)\n",
    "- Ensemble multiple models\n",
    "- Add more sophisticated text preprocessing\n",
    "- Use class weights to handle imbalanced data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
