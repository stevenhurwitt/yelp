{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark read s3 json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported modules.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window, Row, SparkSession\n",
    "\n",
    "import psycopg2\n",
    "import pprint\n",
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent = 3)\n",
    "print('imported modules.')\n",
    "\n",
    "# Set Java home environment variable\n",
    "# os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/temurin-8.jdk/Contents/Home'  # Update this path to match your Java installation\n",
    "\n",
    "# read creds.json\n",
    "with open(\"creds.json\", \"r\") as f:\n",
    "    creds = json.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop any existing Spark session\n",
    "if 'spark' in locals():\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/11 18:56:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Create Spark session with required configurations\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"YelpAnalysis\") \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", creds[\"aws_client\"]) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", creds[\"aws_secret\"]) \\\n",
    "        .config(\"spark.jars.packages\", \n",
    "                \"org.apache.hadoop:hadoop-aws:3.3.4,\" + \n",
    "                \"org.apache.hadoop:hadoop-common:3.3.4,\" +\n",
    "                \"org.apache.logging.log4j:log4j-slf4j-impl:2.17.2,\" +\n",
    "                \"org.apache.logging.log4j:log4j-api:2.17.2,\" +\n",
    "                \"org.apache.logging.log4j:log4j-core:2.17.2,\" + \n",
    "                \"org.apache.hadoop:hadoop-client:3.3.4,\" + \n",
    "                \"io.delta:delta-core_2.12:2.4.0,\" + \n",
    "                \"org.postgresql:postgresql:9.4.1212\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "        # .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-common:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-client:3.3.4,io.delta:delta-core_2.12:2.4.0,org.postgresql:postgresql:9.4.1212\") \\\n",
    "        \n",
    "    \n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list s3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files in s3 bucket:\n",
      "\n",
      "yelp_academic_dataset_business.json\n",
      "yelp_academic_dataset_checkin.json\n",
      "yelp_academic_dataset_review.json\n",
      "yelp_academic_dataset_tip.json\n",
      "yelp_academic_dataset_user.json\n"
     ]
    }
   ],
   "source": [
    "client = boto3.client('s3',\n",
    "                    aws_access_key_id=creds[\"aws_client\"],\n",
    "                    aws_secret_access_key= creds[\"aws_secret\"])\n",
    "bucket = \"yelp-stevenhurwitt-2\"\n",
    "file = \"yelp_academic_dataset_business.json\"\n",
    "\n",
    "bucket_meta = client.list_objects(Bucket = 'yelp-stevenhurwitt-2')\n",
    "print('files in s3 bucket:')\n",
    "print('')\n",
    "for c in bucket_meta['Contents']:\n",
    "    print(c['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark read json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schemas for each dataset\n",
    "checkin_schema = StructType([\n",
    "    StructField(\"business_id\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True)\n",
    "])\n",
    "\n",
    "review_schema = StructType([\n",
    "    StructField(\"review_id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"business_id\", StringType(), True),\n",
    "    StructField(\"stars\", FloatType(), True),\n",
    "    StructField(\"useful\", IntegerType(), True),\n",
    "    StructField(\"funny\", IntegerType(), True),\n",
    "    StructField(\"cool\", IntegerType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "tip_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"business_id\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"date\", TimestampType(), True),\n",
    "    StructField(\"compliment_count\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "user_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"review_count\", IntegerType(), True),\n",
    "    StructField(\"yelping_since\", TimestampType(), True),\n",
    "    StructField(\"friends\", StringType(), True),\n",
    "    StructField(\"useful\", IntegerType(), True),\n",
    "    StructField(\"funny\", IntegerType(), True),\n",
    "    StructField(\"cool\", IntegerType(), True),\n",
    "    StructField(\"fans\", IntegerType(), True),\n",
    "    StructField(\"elite\", StringType(), True),\n",
    "    StructField(\"average_stars\", FloatType(), True),\n",
    "    StructField(\"compliment_hot\", IntegerType(), True),\n",
    "    StructField(\"compliment_more\", IntegerType(), True),\n",
    "    StructField(\"compliment_profile\", IntegerType(), True),\n",
    "    StructField(\"compliment_cute\", IntegerType(), True),\n",
    "    StructField(\"compliment_list\", IntegerType(), True),\n",
    "    StructField(\"compliment_note\", IntegerType(), True),\n",
    "    StructField(\"compliment_plain\", IntegerType(), True),\n",
    "    StructField(\"compliment_cool\", IntegerType(), True),\n",
    "    StructField(\"compliment_funny\", IntegerType(), True),\n",
    "    StructField(\"compliment_writer\", IntegerType(), True),\n",
    "    StructField(\"compliment_photos\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "business_schema = StructType([\n",
    "    StructField(\"business_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"postal_code\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"stars\", DoubleType(), True),\n",
    "    StructField(\"review_count\", IntegerType(), True),\n",
    "    StructField(\"is_open\", IntegerType(), True),\n",
    "    StructField(\"attributes\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"categories\", StringType(), True),\n",
    "    StructField(\"hours\", MapType(StringType(), StringType()), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load yelp dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataframes\n",
    "def load_yelp_dataframes(base_path):\n",
    "    try:\n",
    "        # Load each dataset\n",
    "        df_business = spark.read.schema(business_schema)\\\n",
    "            .json(f\"{base_path}/yelp_academic_dataset_business.json\")\n",
    "        \n",
    "        df_checkin = spark.read.schema(checkin_schema)\\\n",
    "            .json(f\"{base_path}/yelp_academic_dataset_checkin.json\")\n",
    "        \n",
    "        df_review = spark.read.schema(review_schema)\\\n",
    "            .json(f\"{base_path}/yelp_academic_dataset_review.json\")\n",
    "        \n",
    "        df_tip = spark.read.schema(tip_schema)\\\n",
    "            .json(f\"{base_path}/yelp_academic_dataset_tip.json\")\n",
    "        \n",
    "        df_user = spark.read.schema(user_schema)\\\n",
    "            .json(f\"{base_path}/yelp_academic_dataset_user.json\")\n",
    "        \n",
    "        # Print basic info about loaded dataframes\n",
    "        print(f\"Businesses: {df_business.count()} rows\")\n",
    "        print(f\"Checkins: {df_checkin.count()} rows\")\n",
    "        print(f\"Reviews: {df_review.count()} rows\")\n",
    "        print(f\"Tips: {df_tip.count()} rows\")\n",
    "        print(f\"Users: {df_user.count()} rows\")\n",
    "        \n",
    "        return df_business, df_checkin, df_review, df_tip, df_user\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataframes: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data\n",
    "\n",
    "http://localhost:4040/jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Businesses: 150346 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkins: 131930 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews: 6990280 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: 908915 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=====================================================>  (25 + 1) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 1987897 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "base_path = \"s3a://yelp-stevenhurwitt-2/\"  # Update this path\n",
    "df_business, df_checkin, df_review, df_tip, df_user = load_yelp_dataframes(base_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verify schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkin Schema:\n",
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n",
      "\n",
      "Review Schema:\n",
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- stars: float (nullable = true)\n",
      " |-- useful: integer (nullable = true)\n",
      " |-- funny: integer (nullable = true)\n",
      " |-- cool: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n",
      "\n",
      "Tip Schema:\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- compliment_count: integer (nullable = true)\n",
      "\n",
      "\n",
      "User Schema:\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- review_count: integer (nullable = true)\n",
      " |-- yelping_since: timestamp (nullable = true)\n",
      " |-- friends: string (nullable = true)\n",
      " |-- useful: integer (nullable = true)\n",
      " |-- funny: integer (nullable = true)\n",
      " |-- cool: integer (nullable = true)\n",
      " |-- fans: integer (nullable = true)\n",
      " |-- elite: string (nullable = true)\n",
      " |-- average_stars: float (nullable = true)\n",
      " |-- compliment_hot: integer (nullable = true)\n",
      " |-- compliment_more: integer (nullable = true)\n",
      " |-- compliment_profile: integer (nullable = true)\n",
      " |-- compliment_cute: integer (nullable = true)\n",
      " |-- compliment_list: integer (nullable = true)\n",
      " |-- compliment_note: integer (nullable = true)\n",
      " |-- compliment_plain: integer (nullable = true)\n",
      " |-- compliment_cool: integer (nullable = true)\n",
      " |-- compliment_funny: integer (nullable = true)\n",
      " |-- compliment_writer: integer (nullable = true)\n",
      " |-- compliment_photos: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify schemas\n",
    "print(\"\\nCheckin Schema:\")\n",
    "df_checkin.printSchema()\n",
    "print(\"\\nReview Schema:\")\n",
    "df_review.printSchema()\n",
    "print(\"\\nTip Schema:\")\n",
    "df_tip.printSchema()\n",
    "print(\"\\nUser Schema:\")\n",
    "df_user.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write to delta on s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/11 19:57:29 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote reviews data to Delta table at s3a://yelp-stevenhurwitt-2/reviews/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:========>                                                (4 + 4) / 26]\r"
     ]
    }
   ],
   "source": [
    "def write_to_s3_delta(df, bucket, folder, partition_cols=None):\n",
    "    \"\"\"\n",
    "    Write DataFrame to S3 in Delta format\n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "        bucket: S3 bucket name\n",
    "        folder: Folder name for the dataset\n",
    "        partition_cols: List of columns to partition by (optional)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        writer = df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "        \n",
    "        if partition_cols:\n",
    "            writer = writer.partitionBy(partition_cols)\n",
    "            \n",
    "        path = f\"s3a://{bucket}/{folder}/\"\n",
    "        writer.save(path)\n",
    "        print(f\"Successfully wrote {folder} data to Delta table at {path}\")\n",
    "        \n",
    "        # Create Delta table if it doesn't exist\n",
    "        spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS delta.`{path}`\n",
    "        USING DELTA\n",
    "        \"\"\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing {folder} to Delta: {str(e)}\")\n",
    "\n",
    "# Usage example:\n",
    "bucket = \"yelp-stevenhurwitt-2\"\n",
    "\n",
    "try:\n",
    "    # Reviews - partition by year and month\n",
    "    df_review = df_review.withColumn(\n",
    "        \"year\", year(\"date\")\n",
    "    ).withColumn(\n",
    "        \"month\", month(\"date\")\n",
    "    )\n",
    "    write_to_s3_delta(df_review, bucket, \"reviews\", [\"year\", \"month\"])\n",
    "\n",
    "    # Users - no partitioning\n",
    "    write_to_s3_delta(df_user, bucket, \"users\")\n",
    "\n",
    "    # Tips - partition by year\n",
    "    df_tip = df_tip.withColumn(\"year\", year(\"date\"))\n",
    "    write_to_s3_delta(df_tip, bucket, \"tips\", [\"year\"])\n",
    "\n",
    "    # Checkins - no partitioning\n",
    "    write_to_s3_delta(df_checkin, bucket, \"checkins\")\n",
    "\n",
    "    # Business - partition by state\n",
    "    write_to_s3_delta(df_business, bucket, \"business\", [\"state\"])\n",
    "\n",
    "    print(\"All datasets written to Delta format successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in write operations: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business - partition by state\n",
    "write_to_s3_delta(df_business, bucket, \"business\", [\"state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Delta table\n",
    "df = spark.read.format(\"delta\").load(\"s3a://yelp-stevenhurwitt-2/reviews\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write to postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_postgres_tables():\n",
    "    \"\"\"Create PostgreSQL tables in correct dependency order\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=creds[\"postgres_host\"],\n",
    "            database=creds[\"postgres_db\"],\n",
    "            user=creds[\"postgres_user\"],\n",
    "            password=creds[\"postgres_password\"]\n",
    "        )\n",
    "        \n",
    "        with conn.cursor() as cur:\n",
    "            # Read SQL statements\n",
    "            with open('create_tables.sql', 'r') as f:\n",
    "                sql = f.read()\n",
    "                \n",
    "            # Execute statements\n",
    "            cur.execute(sql)\n",
    "            conn.commit()\n",
    "            print(\"Tables created successfully\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating tables: {str(e)}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Execute table creation\n",
    "create_postgres_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write to postgres from spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_postgres(df, table_name):\n",
    "    \"\"\"\n",
    "    Write Spark DataFrame to PostgreSQL\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "        table_name: Name of the target PostgreSQL table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # PostgreSQL connection properties\n",
    "        postgres_props = {\n",
    "            \"url\": f\"jdbc:postgresql://{creds['postgres_host']}:5433/{creds['postgres_db']}\",\n",
    "            \"driver\": \"org.postgresql.Driver\",\n",
    "            \"user\": creds[\"postgres_user\"],\n",
    "            \"password\": creds[\"postgres_password\"]\n",
    "        }\n",
    "        \n",
    "        # Write DataFrame to PostgreSQL\n",
    "        df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", postgres_props[\"url\"]) \\\n",
    "            .option(\"driver\", postgres_props[\"driver\"]) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"user\", postgres_props[\"user\"]) \\\n",
    "            .option(\"password\", postgres_props[\"password\"]) \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "            \n",
    "        print(f\"Successfully wrote {table_name} to PostgreSQL\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing {table_name} to PostgreSQL: {str(e)}\")\n",
    "\n",
    "# Write all datasets to PostgreSQL\n",
    "try:\n",
    "    # Add PostgreSQL JDBC driver to Spark\n",
    "    spark.sparkContext.addJar(\"postgresql-42.2.23.jar\")\n",
    "    \n",
    "    # Write each dataset\n",
    "    write_to_postgres(df_business, \"business\")\n",
    "    write_to_postgres(df_review, \"review\")\n",
    "    write_to_postgres(df_user, \"user_profile\")  # using user_profile since user is reserved\n",
    "    write_to_postgres(df_tip, \"tip\")\n",
    "    write_to_postgres(df_checkin, \"checkin\")\n",
    "    \n",
    "    print(\"All datasets written to PostgreSQL successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in PostgreSQL write operations: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aws athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_s3_delta_with_manifest(df, bucket, folder, partition_cols=None):\n",
    "    \"\"\"\n",
    "    Write DataFrame to S3 in Delta format and generate manifest for Athena\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Write Delta table\n",
    "        path = f\"s3a://{bucket}/{folder}/\"\n",
    "        \n",
    "        writer = df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "        \n",
    "        if partition_cols:\n",
    "            writer = writer.partitionBy(partition_cols)\n",
    "            \n",
    "        writer.save(path)\n",
    "        \n",
    "        # Generate manifest for Athena\n",
    "        spark.sql(f\"\"\"\n",
    "        GENERATE symlink_format_manifest FOR TABLE delta.`{path}`\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"Successfully wrote {folder} data and manifest to {path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing {folder}: {str(e)}\")\n",
    "\n",
    "# AWS Glue catalog registration function\n",
    "def register_with_glue(database_name, table_name, s3_path, partition_cols=None):\n",
    "    \"\"\"\n",
    "    Register Delta table with AWS Glue catalog\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get table schema from Spark\n",
    "        schema = spark.read.format(\"delta\").load(s3_path).schema\n",
    "        \n",
    "        # Convert Spark schema to Glue columns\n",
    "        glue_columns = []\n",
    "        for field in schema.fields:\n",
    "            glue_columns.append({\n",
    "                'Name': field.name,\n",
    "                'Type': field.dataType.simpleString()\n",
    "            })\n",
    "            \n",
    "        # Create Glue client\n",
    "        glue = boto3.client('glue',\n",
    "                           aws_access_key_id=creds['aws_client'],\n",
    "                           aws_secret_access_key=creds['aws_secret'])\n",
    "        \n",
    "        # Create table definition\n",
    "        table_input = {\n",
    "            'Name': table_name,\n",
    "            'StorageDescriptor': {\n",
    "                'Columns': glue_columns,\n",
    "                'Location': s3_path,\n",
    "                'InputFormat': 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat',\n",
    "                'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
    "                'SerdeInfo': {\n",
    "                    'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
    "                }\n",
    "            },\n",
    "            'TableType': 'EXTERNAL_TABLE'\n",
    "        }\n",
    "        \n",
    "        if partition_cols:\n",
    "            table_input['PartitionKeys'] = [{'Name': col, 'Type': 'string'} for col in partition_cols]\n",
    "        \n",
    "        # Create or update table in Glue\n",
    "        glue.create_table(\n",
    "            DatabaseName=database_name,\n",
    "            TableInput=table_input\n",
    "        )\n",
    "        \n",
    "        print(f\"Registered table {table_name} in Glue catalog\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error registering with Glue: {str(e)}\")\n",
    "\n",
    "# Usage example:\n",
    "bucket = \"yelp-stevenhurwitt-2\"\n",
    "database_name = \"yelp_analytics\"\n",
    "\n",
    "try:\n",
    "    # Reviews - partition by year and month\n",
    "    df_review = df_review.withColumn(\"year\", year(\"date\")).withColumn(\"month\", month(\"date\"))\n",
    "    write_to_s3_delta_with_manifest(df_review, bucket, \"reviews\", [\"year\", \"month\"])\n",
    "    register_with_glue(database_name, \"reviews\", f\"s3://{bucket}/reviews\", [\"year\", \"month\"])\n",
    "\n",
    "    # Users\n",
    "    write_to_s3_delta_with_manifest(df_user, bucket, \"users\")\n",
    "    register_with_glue(database_name, \"users\", f\"s3://{bucket}/users\")\n",
    "\n",
    "    # Tips - partition by year\n",
    "    df_tip = df_tip.withColumn(\"year\", year(\"date\"))\n",
    "    write_to_s3_delta_with_manifest(df_tip, bucket, \"tips\", [\"year\"])\n",
    "    register_with_glue(database_name, \"tips\", f\"s3://{bucket}/tips\", [\"year\"])\n",
    "\n",
    "    # Checkins\n",
    "    write_to_s3_delta_with_manifest(df_checkin, bucket, \"checkins\")\n",
    "    register_with_glue(database_name, \"checkins\", f\"s3://{bucket}/checkins\")\n",
    "\n",
    "    print(\"All tables written and registered successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in operations: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    \"\"\"\n",
    "    reads a yelp .json file from s3 bucket.\n",
    "\n",
    "    keyword arguments:\n",
    "    filename - name of file (str)\n",
    "\n",
    "    returns: json_file (json)\n",
    "    \"\"\"\n",
    "\n",
    "    bucket = \"yelp-dataset-stevenhurwitt\"\n",
    "    file = \"raw/yelp_academic_dataset_business.json\"\n",
    "    response = client.get_object(Bucket = bucket, Key = file)\n",
    "    \n",
    "    file_content = response['Body'].read().decode('utf-8')\n",
    "    json_file = json.loads(\"[\" + file_content.replace(\"}\\n{\", \"},\\n{\") + \"]\")\n",
    "    return(json_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read json files from s3.\n"
     ]
    }
   ],
   "source": [
    "business_file = read_json(\"raw/yelp_academic_dataset_business.json\")\n",
    "checkin_file = read_json(\"raw/yelp_academic_dataset_checkin.json\")\n",
    "review_file = read_json(\"raw/yelp_academic_dataset_review.json\")\n",
    "tip_file = read_json(\"raw/yelp_academic_dataset_tip.json\")\n",
    "user_file = read_json(\"raw/yelp_academic_dataset_user.json\")\n",
    "\n",
    "print(\"read json files from s3.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  'address': '1616 Chapala St, Ste 2',\n",
      "   'attributes': {'ByAppointmentOnly': 'True'},\n",
      "   'business_id': 'Pns2l4eNsfO8kk83dixA6A',\n",
      "   'categories': 'Doctors, Traditional Chinese Medicine, '\n",
      "                 'Naturopathic/Holistic, Acupuncture, Health & Medical, '\n",
      "                 'Nutritionists',\n",
      "   'city': 'Santa Barbara',\n",
      "   'hours': None,\n",
      "   'is_open': 0,\n",
      "   'latitude': 34.4266787,\n",
      "   'longitude': -119.7111968,\n",
      "   'name': 'Abby Rappoport, LAC, CMQ',\n",
      "   'postal_code': '93101',\n",
      "   'review_count': 7,\n",
      "   'stars': 5.0,\n",
      "   'state': 'CA'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(review_file[0])\n",
    "\n",
    "# {  'address': '1616 Chapala St, Ste 2',\n",
    "#    'attributes': {'ByAppointmentOnly': 'True'},\n",
    "#    'business_id': 'Pns2l4eNsfO8kk83dixA6A',\n",
    "#    'categories': 'Doctors, Traditional Chinese Medicine, '\n",
    "#                  'Naturopathic/Holistic, Acupuncture, Health & Medical, '\n",
    "#                  'Nutritionists',\n",
    "#    'city': 'Santa Barbara',\n",
    "#    'hours': None,\n",
    "#    'is_open': 0,\n",
    "#    'latitude': 34.4266787,\n",
    "#    'longitude': -119.7111968,\n",
    "#    'name': 'Abby Rappoport, LAC, CMQ',\n",
    "#    'postal_code': '93101',\n",
    "#    'review_count': 7,\n",
    "#    'stars': 5.0,\n",
    "#    'state': 'CA'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  'address': '1616 Chapala St, Ste 2',\n",
      "   'attributes': {'ByAppointmentOnly': 'True'},\n",
      "   'business_id': 'Pns2l4eNsfO8kk83dixA6A',\n",
      "   'categories': 'Doctors, Traditional Chinese Medicine, '\n",
      "                 'Naturopathic/Holistic, Acupuncture, Health & Medical, '\n",
      "                 'Nutritionists',\n",
      "   'city': 'Santa Barbara',\n",
      "   'hours': None,\n",
      "   'is_open': 0,\n",
      "   'latitude': 34.4266787,\n",
      "   'longitude': -119.7111968,\n",
      "   'name': 'Abby Rappoport, LAC, CMQ',\n",
      "   'postal_code': '93101',\n",
      "   'review_count': 7,\n",
      "   'stars': 5.0,\n",
      "   'state': 'CA'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(tip_file[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  'address': '1616 Chapala St, Ste 2',\n",
      "   'attributes': {'ByAppointmentOnly': 'True'},\n",
      "   'business_id': 'Pns2l4eNsfO8kk83dixA6A',\n",
      "   'categories': 'Doctors, Traditional Chinese Medicine, '\n",
      "                 'Naturopathic/Holistic, Acupuncture, Health & Medical, '\n",
      "                 'Nutritionists',\n",
      "   'city': 'Santa Barbara',\n",
      "   'hours': None,\n",
      "   'is_open': 0,\n",
      "   'latitude': 34.4266787,\n",
      "   'longitude': -119.7111968,\n",
      "   'name': 'Abby Rappoport, LAC, CMQ',\n",
      "   'postal_code': '93101',\n",
      "   'review_count': 7,\n",
      "   'stars': 5.0,\n",
      "   'state': 'CA'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(user_file[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checkin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  'address': '1616 Chapala St, Ste 2',\n",
      "   'attributes': {'ByAppointmentOnly': 'True'},\n",
      "   'business_id': 'Pns2l4eNsfO8kk83dixA6A',\n",
      "   'categories': 'Doctors, Traditional Chinese Medicine, '\n",
      "                 'Naturopathic/Holistic, Acupuncture, Health & Medical, '\n",
      "                 'Nutritionists',\n",
      "   'city': 'Santa Barbara',\n",
      "   'hours': None,\n",
      "   'is_open': 0,\n",
      "   'latitude': 34.4266787,\n",
      "   'longitude': -119.7111968,\n",
      "   'name': 'Abby Rappoport, LAC, CMQ',\n",
      "   'postal_code': '93101',\n",
      "   'review_count': 7,\n",
      "   'stars': 5.0,\n",
      "   'state': 'CA'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(checkin_file[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  'address': '1616 Chapala St, Ste 2',\n",
      "   'attributes': {'ByAppointmentOnly': 'True'},\n",
      "   'business_id': 'Pns2l4eNsfO8kk83dixA6A',\n",
      "   'categories': 'Doctors, Traditional Chinese Medicine, '\n",
      "                 'Naturopathic/Holistic, Acupuncture, Health & Medical, '\n",
      "                 'Nutritionists',\n",
      "   'city': 'Santa Barbara',\n",
      "   'hours': None,\n",
      "   'is_open': 0,\n",
      "   'latitude': 34.4266787,\n",
      "   'longitude': -119.7111968,\n",
      "   'name': 'Abby Rappoport, LAC, CMQ',\n",
      "   'postal_code': '93101',\n",
      "   'review_count': 7,\n",
      "   'stars': 5.0,\n",
      "   'state': 'CA'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(business_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dynamo resource.\n"
     ]
    }
   ],
   "source": [
    "dynamodb = boto3.resource('dynamodb', endpoint_url=\"https://us-east-2.console.aws.amazon.com?arn=arn:aws:dynamodb:us-east-2134132211607:8000\")\n",
    "print('created dynamo resource.')\n",
    "\n",
    "# try:\n",
    "#     yelp_business = dynamodb.create_table(\n",
    "#             TableName='yelp.business',\n",
    "#             KeySchema=[\n",
    "#                 {\n",
    "#                     'AttributeName': 'business_id',\n",
    "#                     'KeyType': 'HASH'  # Partition key\n",
    "#                 }\n",
    "#             ],\n",
    "#             AttributeDefinitions=[\n",
    "#                 {\n",
    "#                     'AttributeName': 'name',\n",
    "#                     'AttributeType': 'S'\n",
    "#                 }\n",
    "#             ],\n",
    "#             ProvisionedThroughput={\n",
    "#                 'ReadCapacityUnits': 25,\n",
    "#                 'WriteCapacityUnits': 20\n",
    "#             }\n",
    "#         )\n",
    "#     print('created dynamo table.')\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"exception: {e}\")\n",
    "#     print(\"failed to create dynamo table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp-dataset-stevenhurwitt\n"
     ]
    }
   ],
   "source": [
    "print(bucket) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw/yelp_academic_dataset_business.json\n"
     ]
    }
   ],
   "source": [
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150346\n",
      "150346\n"
     ]
    }
   ],
   "source": [
    "print(len(business_file))\n",
    "print(len(checkin_file))\n",
    "# review_file = read_json(\"raw/yelp_academic_dataset_review.json\")\n",
    "# tip_file = read_json(\"raw/yelp_academic_dataset_tip.json\")\n",
    "# user_file = read_json(\"raw/yelp_academic_dataset_user.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business json file has 150346 records with size of 1.28316 mb.\n",
      "tip json file has 150346 records with size of 1.28316 mb.\n",
      "user json file has 150346 records with size of 1.28316 mb.\n",
      "review json file has 150346 records with size of 1.28316 mb.\n",
      "checkin json file has 150346 records with size of 1.28316 mb.\n"
     ]
    }
   ],
   "source": [
    "print('business json file has {} records with size of {} mb.'.format(len(business_file), sys.getsizeof(business_file)/1000000))\n",
    "print('tip json file has {} records with size of {} mb.'.format(len(tip_file), sys.getsizeof(tip_file)/1000000))\n",
    "print('user json file has {} records with size of {} mb.'.format(len(user_file), sys.getsizeof(user_file)/1000000))\n",
    "print('review json file has {} records with size of {} mb.'.format(len(review_file), sys.getsizeof(review_file)/1000000))\n",
    "print('checkin json file has {} records with size of {} mb.'.format(len(checkin_file), sys.getsizeof(checkin_file)/1000000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas = business.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_df = df_pandas.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(html_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gcloud auth login --no-launch-browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
