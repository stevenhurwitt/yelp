{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark read s3 delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported modules.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window, Row, SparkSession\n",
    "\n",
    "import pprint\n",
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent = 3)\n",
    "print('imported modules.')\n",
    "\n",
    "# Set Java home environment variable\n",
    "os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/temurin-8.jdk/Contents/Home'  # Update this path to match your Java installation\n",
    "\n",
    "# read creds.json\n",
    "with open(\"../creds.json\", \"r\") as f:\n",
    "    creds = json.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/homebrew/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/stevenhurwitt/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/stevenhurwitt/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.apache.hadoop#hadoop-common added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-19ef097f-881c-4041-bc0b-9a76f0ad508f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.hadoop#hadoop-common;3.3.4 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.4 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in local-m2-cache\n",
      "\tfound com.google.guava#guava;27.0-jre in local-m2-cache\n",
      "\tfound com.google.guava#failureaccess;1.0 in local-m2-cache\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in local-m2-cache\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in local-m2-cache\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in local-m2-cache\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in local-m2-cache\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in local-m2-cache\n",
      "\tfound commons-cli#commons-cli;1.2 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in local-m2-cache\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in local-m2-cache\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in local-m2-cache\n",
      "\tfound commons-logging#commons-logging;1.1.3 in local-m2-cache\n",
      "\tfound commons-codec#commons-codec;1.15 in local-m2-cache\n",
      "\tfound commons-io#commons-io;2.8.0 in local-m2-cache\n",
      "\tfound commons-net#commons-net;3.6 in local-m2-cache\n",
      "\tfound commons-collections#commons-collections;3.2.2 in local-m2-cache\n",
      "\tfound javax.servlet#javax.servlet-api;3.1.0 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-server;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-http;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-io;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-servlet;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-security;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-webapp;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-xml;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound com.sun.jersey#jersey-core;1.19 in local-m2-cache\n",
      "\tfound javax.ws.rs#jsr311-api;1.1.1 in local-m2-cache\n",
      "\tfound com.sun.jersey#jersey-servlet;1.19 in local-m2-cache\n",
      "\tfound com.sun.jersey#jersey-server;1.19 in local-m2-cache\n",
      "\tfound com.sun.jersey#jersey-json;1.19 in local-m2-cache\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in local-m2-cache\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in local-m2-cache\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in local-m2-cache\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in local-m2-cache\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in local-m2-cache\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in local-m2-cache\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in local-m2-cache\n",
      "\tfound ch.qos.reload4j#reload4j;1.2.22 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.4 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-configuration2;2.1.1 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-lang3;3.12.0 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-text;1.4 in local-m2-cache\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in local-m2-cache\n",
      "\tfound org.slf4j#slf4j-reload4j;1.7.36 in central\n",
      "\tfound org.apache.avro#avro;1.7.7 in local-m2-cache\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in local-m2-cache\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-compress;1.21 in local-m2-cache\n",
      "\tfound com.google.re2j#re2j;1.1 in local-m2-cache\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in local-m2-cache\n",
      "\tfound com.google.code.gson#gson;2.8.9 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-auth;3.3.4 in central\n",
      "\tfound com.nimbusds#nimbus-jose-jwt;9.8.1 in local-m2-cache\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in local-m2-cache\n",
      "\tfound net.minidev#json-smart;2.4.7 in local-m2-cache\n",
      "\tfound net.minidev#accessors-smart;2.4.7 in local-m2-cache\n",
      "\tfound org.ow2.asm#asm;5.0.4 in local-m2-cache\n",
      "\tfound org.apache.zookeeper#zookeeper;3.5.6 in local-m2-cache\n",
      "\tfound org.apache.zookeeper#zookeeper-jute;3.5.6 in local-m2-cache\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in local-m2-cache\n",
      "\tfound org.apache.curator#curator-framework;4.2.0 in local-m2-cache\n",
      "\tfound org.apache.curator#curator-client;4.2.0 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-client;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-config;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-core;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-pkix;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-asn1;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-util;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-common;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-crypto;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-util;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#token-provider;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-admin;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-server;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-identity;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-xdr;1.0.1 in local-m2-cache\n",
      "\tfound com.jcraft#jsch;0.1.55 in local-m2-cache\n",
      "\tfound org.apache.curator#curator-recipes;4.2.0 in local-m2-cache\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.7 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.7 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.7 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in local-m2-cache\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;5.3.0 in local-m2-cache\n",
      "\tfound dnsjava#dnsjava;2.1.7 in local-m2-cache\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in local-m2-cache\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in local-m2-cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java gateway process exited before sending its port number\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: resolution report :: resolve 3388ms :: artifacts dl 84ms\n",
      "\t:: modules in use:\n",
      "\tch.qos.reload4j#reload4j;1.2.22 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;5.3.0 from local-m2-cache in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from local-m2-cache in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from local-m2-cache in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from local-m2-cache in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from local-m2-cache in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from local-m2-cache in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from local-m2-cache in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from local-m2-cache in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from local-m2-cache in [default]\n",
      "\tcom.google.re2j#re2j;1.1 from local-m2-cache in [default]\n",
      "\tcom.jcraft#jsch;0.1.55 from local-m2-cache in [default]\n",
      "\tcom.nimbusds#nimbus-jose-jwt;9.8.1 from local-m2-cache in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.19 from local-m2-cache in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.19 from local-m2-cache in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.19 from local-m2-cache in [default]\n",
      "\tcom.sun.jersey#jersey-servlet;1.19 from local-m2-cache in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from local-m2-cache in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from local-m2-cache in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.4 from local-m2-cache in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from local-m2-cache in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from local-m2-cache in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from local-m2-cache in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from local-m2-cache in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from local-m2-cache in [default]\n",
      "\tcommons-net#commons-net;3.6 from local-m2-cache in [default]\n",
      "\tdnsjava#dnsjava;2.1.7 from local-m2-cache in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from local-m2-cache in [default]\n",
      "\tjavax.servlet#javax.servlet-api;3.1.0 from local-m2-cache in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from local-m2-cache in [default]\n",
      "\tjavax.ws.rs#jsr311-api;1.1.1 from local-m2-cache in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from local-m2-cache in [default]\n",
      "\tnet.minidev#accessors-smart;2.4.7 from local-m2-cache in [default]\n",
      "\tnet.minidev#json-smart;2.4.7 from local-m2-cache in [default]\n",
      "\torg.apache.avro#avro;1.7.7 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-compress;1.21 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-configuration2;2.1.1 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-lang3;3.12.0 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-text;1.4 from local-m2-cache in [default]\n",
      "\torg.apache.curator#curator-client;4.2.0 from local-m2-cache in [default]\n",
      "\torg.apache.curator#curator-framework;4.2.0 from local-m2-cache in [default]\n",
      "\torg.apache.curator#curator-recipes;4.2.0 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 from local-m2-cache in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from local-m2-cache in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-admin;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-client;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-common;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-core;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-crypto;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-identity;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-server;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-simplekdc;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-util;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-asn1;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-config;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-pkix;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-util;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-xdr;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#token-provider;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from local-m2-cache in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.5.6 from local-m2-cache in [default]\n",
      "\torg.apache.zookeeper#zookeeper-jute;3.5.6 from local-m2-cache in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from local-m2-cache in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from local-m2-cache in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from local-m2-cache in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from local-m2-cache in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from local-m2-cache in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from local-m2-cache in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from local-m2-cache in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-http;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-io;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-security;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-server;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-servlet;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-webapp;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-xml;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from local-m2-cache in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from local-m2-cache in [default]\n",
      "\torg.slf4j#slf4j-reload4j;1.7.36 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from local-m2-cache in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   95  |   0   |   0   |   0   ||   95  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: WARNINGS\n",
      "\t\t[NOT FOUND  ] org.apache.zookeeper#zookeeper-jute;3.5.6!zookeeper-jute.jar (5ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/Users/stevenhurwitt/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.6/zookeeper-jute-3.5.6.jar\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\t\t::              FAILED DOWNLOADS            ::\n",
      "\n",
      "\t\t:: ^ see resolution messages for details  ^ ::\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\t\t:: org.apache.zookeeper#zookeeper-jute;3.5.6!zookeeper-jute.jar\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      "Exception in thread \"main\" java.lang.RuntimeException: [download failed: org.apache.zookeeper#zookeeper-jute;3.5.6!zookeeper-jute.jar]\n",
      "\tat org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1456)\n",
      "\tat org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:308)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Create Spark session with required configurations\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"YelpAnalysis\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", creds[\"aws_client\"]) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", creds[\"aws_secret\"]) \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-common:3.3.4\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "        # .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-common:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-client:3.3.4,io.delta:delta-core_2.12:2.3.0,org.postgresql:postgresql:9.4.1212\") \\\n",
    "        \n",
    "    \n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files in s3 bucket:\n",
      "\n",
      "yelp_academic_dataset_business.json\n",
      "yelp_academic_dataset_checkin.json\n",
      "yelp_academic_dataset_tip.json\n"
     ]
    }
   ],
   "source": [
    "client = boto3.client('s3')\n",
    "bucket = \"yelp-stevenhurwitt-2\"\n",
    "file = \"yelp_academic_dataset_business.json\"\n",
    "\n",
    "bucket_meta = client.list_objects(Bucket = 'yelp-stevenhurwitt-2')\n",
    "print('files in s3 bucket:')\n",
    "print('')\n",
    "for c in bucket_meta['Contents']:\n",
    "    print(c['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark read json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/11 14:31:03 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://yelp-stevenhurwitt-2/yelp_academic_dataset_business.json.\n",
      "java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n",
      "\t... 26 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o59.load.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 24\u001b[0m\n\u001b[1;32m      2\u001b[0m business_schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m      3\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbusiness_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      4\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhours\u001b[39m\u001b[38;5;124m\"\u001b[39m, MapType(StringType(), StringType()), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m ])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Read JSON file from S3\u001b[39;00m\n\u001b[1;32m     20\u001b[0m business_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbusiness_schema\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultiLine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfalse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3a://yelp-stevenhurwitt-2/yelp_academic_dataset_business.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/sql/readwriter.py:307\u001b[0m, in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtable\u001b[39m(\u001b[38;5;28mself\u001b[39m, tableName: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.4.0\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m \n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m    tableName : str\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03m        string, name of the table.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    >>> df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m    >>> df.createOrReplaceTempView('tmpTable')\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;03m    >>> spark.read.table('tmpTable').dtypes\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03m    [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mtable(tableName))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m-> 1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o59.load.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n"
     ]
    }
   ],
   "source": [
    "# Define schema for Yelp business data\n",
    "business_schema = StructType([\n",
    "    StructField(\"business_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"postal_code\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"stars\", DoubleType(), True),\n",
    "    StructField(\"review_count\", IntegerType(), True),\n",
    "    StructField(\"is_open\", IntegerType(), True),\n",
    "    StructField(\"attributes\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"categories\", StringType(), True),\n",
    "    StructField(\"hours\", MapType(StringType(), StringType()), True)\n",
    "])\n",
    "\n",
    "# Read JSON file from S3\n",
    "business_df = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(business_schema) \\\n",
    "    .option(\"multiLine\", \"false\") \\\n",
    "    .load(\"s3a://yelp-stevenhurwitt-2/yelp_academic_dataset_business.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verify data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'business_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Verify the data load\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRow count:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mbusiness_df\u001b[49m\u001b[38;5;241m.\u001b[39mcount())\n\u001b[1;32m      3\u001b[0m business_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'business_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Verify the data load\n",
    "print(\"Row count:\", business_df.count())\n",
    "business_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    \"\"\"\n",
    "    reads a yelp .json file from s3 bucket.\n",
    "\n",
    "    keyword arguments:\n",
    "    filename - name of file (str)\n",
    "\n",
    "    returns: json_file (json)\n",
    "    \"\"\"\n",
    "\n",
    "    bucket = \"yelp-dataset-stevenhurwitt\"\n",
    "    file = \"raw/yelp_academic_dataset_business.json\"\n",
    "    response = client.get_object(Bucket = bucket, Key = file)\n",
    "    \n",
    "    file_content = response['Body'].read().decode('utf-8')\n",
    "    json_file = json.loads(\"[\" + file_content.replace(\"}\\n{\", \"},\\n{\") + \"]\")\n",
    "    return(json_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read json files from s3.\n"
     ]
    }
   ],
   "source": [
    "business_file = read_json(\"raw/yelp_academic_dataset_business.json\")\n",
    "checkin_file = read_json(\"raw/yelp_academic_dataset_checkin.json\")\n",
    "review_file = read_json(\"raw/yelp_academic_dataset_review.json\")\n",
    "tip_file = read_json(\"raw/yelp_academic_dataset_tip.json\")\n",
    "user_file = read_json(\"raw/yelp_academic_dataset_user.json\")\n",
    "\n",
    "print(\"read json files from s3.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  'address': '1616 Chapala St, Ste 2',\n",
      "   'attributes': {'ByAppointmentOnly': 'True'},\n",
      "   'business_id': 'Pns2l4eNsfO8kk83dixA6A',\n",
      "   'categories': 'Doctors, Traditional Chinese Medicine, '\n",
      "                 'Naturopathic/Holistic, Acupuncture, Health & Medical, '\n",
      "                 'Nutritionists',\n",
      "   'city': 'Santa Barbara',\n",
      "   'hours': None,\n",
      "   'is_open': 0,\n",
      "   'latitude': 34.4266787,\n",
      "   'longitude': -119.7111968,\n",
      "   'name': 'Abby Rappoport, LAC, CMQ',\n",
      "   'postal_code': '93101',\n",
      "   'review_count': 7,\n",
      "   'stars': 5.0,\n",
      "   'state': 'CA'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(review_file[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  'address': '1616 Chapala St, Ste 2',\n",
      "   'attributes': {'ByAppointmentOnly': 'True'},\n",
      "   'business_id': 'Pns2l4eNsfO8kk83dixA6A',\n",
      "   'categories': 'Doctors, Traditional Chinese Medicine, '\n",
      "                 'Naturopathic/Holistic, Acupuncture, Health & Medical, '\n",
      "                 'Nutritionists',\n",
      "   'city': 'Santa Barbara',\n",
      "   'hours': None,\n",
      "   'is_open': 0,\n",
      "   'latitude': 34.4266787,\n",
      "   'longitude': -119.7111968,\n",
      "   'name': 'Abby Rappoport, LAC, CMQ',\n",
      "   'postal_code': '93101',\n",
      "   'review_count': 7,\n",
      "   'stars': 5.0,\n",
      "   'state': 'CA'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(tip_file[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  'address': '1616 Chapala St, Ste 2',\n",
      "   'attributes': {'ByAppointmentOnly': 'True'},\n",
      "   'business_id': 'Pns2l4eNsfO8kk83dixA6A',\n",
      "   'categories': 'Doctors, Traditional Chinese Medicine, '\n",
      "                 'Naturopathic/Holistic, Acupuncture, Health & Medical, '\n",
      "                 'Nutritionists',\n",
      "   'city': 'Santa Barbara',\n",
      "   'hours': None,\n",
      "   'is_open': 0,\n",
      "   'latitude': 34.4266787,\n",
      "   'longitude': -119.7111968,\n",
      "   'name': 'Abby Rappoport, LAC, CMQ',\n",
      "   'postal_code': '93101',\n",
      "   'review_count': 7,\n",
      "   'stars': 5.0,\n",
      "   'state': 'CA'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(user_file[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checkin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  'address': '1616 Chapala St, Ste 2',\n",
      "   'attributes': {'ByAppointmentOnly': 'True'},\n",
      "   'business_id': 'Pns2l4eNsfO8kk83dixA6A',\n",
      "   'categories': 'Doctors, Traditional Chinese Medicine, '\n",
      "                 'Naturopathic/Holistic, Acupuncture, Health & Medical, '\n",
      "                 'Nutritionists',\n",
      "   'city': 'Santa Barbara',\n",
      "   'hours': None,\n",
      "   'is_open': 0,\n",
      "   'latitude': 34.4266787,\n",
      "   'longitude': -119.7111968,\n",
      "   'name': 'Abby Rappoport, LAC, CMQ',\n",
      "   'postal_code': '93101',\n",
      "   'review_count': 7,\n",
      "   'stars': 5.0,\n",
      "   'state': 'CA'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(checkin_file[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  'address': '1616 Chapala St, Ste 2',\n",
      "   'attributes': {'ByAppointmentOnly': 'True'},\n",
      "   'business_id': 'Pns2l4eNsfO8kk83dixA6A',\n",
      "   'categories': 'Doctors, Traditional Chinese Medicine, '\n",
      "                 'Naturopathic/Holistic, Acupuncture, Health & Medical, '\n",
      "                 'Nutritionists',\n",
      "   'city': 'Santa Barbara',\n",
      "   'hours': None,\n",
      "   'is_open': 0,\n",
      "   'latitude': 34.4266787,\n",
      "   'longitude': -119.7111968,\n",
      "   'name': 'Abby Rappoport, LAC, CMQ',\n",
      "   'postal_code': '93101',\n",
      "   'review_count': 7,\n",
      "   'stars': 5.0,\n",
      "   'state': 'CA'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(business_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dynamo resource.\n"
     ]
    }
   ],
   "source": [
    "dynamodb = boto3.resource('dynamodb', endpoint_url=\"https://us-east-2.console.aws.amazon.com?arn=arn:aws:dynamodb:us-east-2134132211607:8000\")\n",
    "print('created dynamo resource.')\n",
    "\n",
    "# try:\n",
    "#     yelp_business = dynamodb.create_table(\n",
    "#             TableName='yelp.business',\n",
    "#             KeySchema=[\n",
    "#                 {\n",
    "#                     'AttributeName': 'business_id',\n",
    "#                     'KeyType': 'HASH'  # Partition key\n",
    "#                 }\n",
    "#             ],\n",
    "#             AttributeDefinitions=[\n",
    "#                 {\n",
    "#                     'AttributeName': 'name',\n",
    "#                     'AttributeType': 'S'\n",
    "#                 }\n",
    "#             ],\n",
    "#             ProvisionedThroughput={\n",
    "#                 'ReadCapacityUnits': 25,\n",
    "#                 'WriteCapacityUnits': 20\n",
    "#             }\n",
    "#         )\n",
    "#     print('created dynamo table.')\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"exception: {e}\")\n",
    "#     print(\"failed to create dynamo table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp-dataset-stevenhurwitt\n"
     ]
    }
   ],
   "source": [
    "print(bucket) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw/yelp_academic_dataset_business.json\n"
     ]
    }
   ],
   "source": [
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150346\n",
      "150346\n"
     ]
    }
   ],
   "source": [
    "print(len(business_file))\n",
    "print(len(checkin_file))\n",
    "# review_file = read_json(\"raw/yelp_academic_dataset_review.json\")\n",
    "# tip_file = read_json(\"raw/yelp_academic_dataset_tip.json\")\n",
    "# user_file = read_json(\"raw/yelp_academic_dataset_user.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business json file has 150346 records with size of 1.28316 mb.\n",
      "tip json file has 150346 records with size of 1.28316 mb.\n",
      "user json file has 150346 records with size of 1.28316 mb.\n",
      "review json file has 150346 records with size of 1.28316 mb.\n",
      "checkin json file has 150346 records with size of 1.28316 mb.\n"
     ]
    }
   ],
   "source": [
    "print('business json file has {} records with size of {} mb.'.format(len(business_file), sys.getsizeof(business_file)/1000000))\n",
    "print('tip json file has {} records with size of {} mb.'.format(len(tip_file), sys.getsizeof(tip_file)/1000000))\n",
    "print('user json file has {} records with size of {} mb.'.format(len(user_file), sys.getsizeof(user_file)/1000000))\n",
    "print('review json file has {} records with size of {} mb.'.format(len(review_file), sys.getsizeof(review_file)/1000000))\n",
    "print('checkin json file has {} records with size of {} mb.'.format(len(checkin_file), sys.getsizeof(checkin_file)/1000000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas = business.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_df = df_pandas.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(html_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gcloud auth login --no-launch-browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
